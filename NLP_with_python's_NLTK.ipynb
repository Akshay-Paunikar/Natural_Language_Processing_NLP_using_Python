{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) is a field that focuses on making natural human language usable by computer programs. NLTK, or Natural Language Toolkit, is a Python package that you can use for NLP.\n",
    "\n",
    "A lot of the data that you could be analyzing is unstructured data and contains human-readable text. Before you can analyze that data programmatically, you first need to preprocess it. Let's look at the kinds of text preprocessing tasks you can do with NLTK so that you'll be ready to apply them in future projects. You'll also see how to do some basic text analysis and create visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "By tokenizing, you can conveniently split up text by word or by sentence. This will allow you to work with smaller pieces of text that are still relatively coherent and meaningful even outside of the context of the rest of the text. It’s your first step in turning unstructured data into structured data, which is easier to analyze.\n",
    "\n",
    "When you’re analyzing text, you’ll be tokenizing by word and tokenizing by sentence. Here’s what both types of tokenization bring to the table:\n",
    " - <b>Tokenizing by word</b>: Words are like the atoms of natural language. They’re the smallest unit of meaning that still makes sense on its own. Tokenizing your text by word allows you to identify words that come up particularly often.\n",
    " - <b>Tokenizing by sentence</b>: When you tokenize by sentence, you can analyze how those words relate to one another and see more context. Are there a lot of negative words around the word “Python” because the hiring manager doesn’t like Python? Are there more terms from the domain of herpetology than the domain of software development, suggesting that you may be dealing with an entirely different kind of python than you were expecting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import word and sentence tokenize from nltk.tokenize\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today I am very happy to tell you that I have completed a project in NLP. With the use of NLP I extracted information on twitter sentiment analysis. I used that data to segment tweets into positive, negative and neutral tweets. In future I will use it to analyze comments from twitter account.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a sample data for demonstration\n",
    "sample_data = \"\"\"Today I am very happy to tell you that I have completed a project in NLP. With the use of NLP I extracted information on twitter sentiment analysis. I used that data to segment tweets into positive, negative and neutral tweets. In future I will use it to analyze comments from twitter account.\"\"\"\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today I am very happy to tell you that I have completed a project in NLP.',\n",
       " 'With the use of NLP I extracted information on twitter sentiment analysis.',\n",
       " 'I used that data to segment tweets into positive, negative and neutral tweets.',\n",
       " 'In future I will use it to analyze comments from twitter account.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenize\n",
    "sent_tokenize(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today',\n",
       " 'I',\n",
       " 'am',\n",
       " 'very',\n",
       " 'happy',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'that',\n",
       " 'I',\n",
       " 'have',\n",
       " 'completed',\n",
       " 'a',\n",
       " 'project',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'With',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'NLP',\n",
       " 'I',\n",
       " 'extracted',\n",
       " 'information',\n",
       " 'on',\n",
       " 'twitter',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " '.',\n",
       " 'I',\n",
       " 'used',\n",
       " 'that',\n",
       " 'data',\n",
       " 'to',\n",
       " 'segment',\n",
       " 'tweets',\n",
       " 'into',\n",
       " 'positive',\n",
       " ',',\n",
       " 'negative',\n",
       " 'and',\n",
       " 'neutral',\n",
       " 'tweets',\n",
       " '.',\n",
       " 'In',\n",
       " 'future',\n",
       " 'I',\n",
       " 'will',\n",
       " 'use',\n",
       " 'it',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'comments',\n",
       " 'from',\n",
       " 'twitter',\n",
       " 'account',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenize\n",
    "word_tokenize(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOPWORDS filtering\n",
    "Stop words are words that you want to ignore, so you filter them out of your text when you’re processing it. Very common words like 'in', 'is', and 'an' are often used as stop words since they don’t add a lot of meaning to a text in and of themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'today i am very happy to tell you that i have completed a project in nlp. with the use of nlp i extracted information on twitter sentiment analysis. i used that data to segment tweets into positive, negative and neutral tweets. in future i will use it to analyze comments from twitter account.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we will convert our sample data into lower case\n",
    "sample_data_lower = sample_data.lower()\n",
    "sample_data_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today',\n",
       " 'i',\n",
       " 'am',\n",
       " 'very',\n",
       " 'happy',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'that',\n",
       " 'i',\n",
       " 'have',\n",
       " 'completed',\n",
       " 'a',\n",
       " 'project',\n",
       " 'in',\n",
       " 'nlp',\n",
       " '.',\n",
       " 'with',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'nlp',\n",
       " 'i',\n",
       " 'extracted',\n",
       " 'information',\n",
       " 'on',\n",
       " 'twitter',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " '.',\n",
       " 'i',\n",
       " 'used',\n",
       " 'that',\n",
       " 'data',\n",
       " 'to',\n",
       " 'segment',\n",
       " 'tweets',\n",
       " 'into',\n",
       " 'positive',\n",
       " ',',\n",
       " 'negative',\n",
       " 'and',\n",
       " 'neutral',\n",
       " 'tweets',\n",
       " '.',\n",
       " 'in',\n",
       " 'future',\n",
       " 'i',\n",
       " 'will',\n",
       " 'use',\n",
       " 'it',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'comments',\n",
       " 'from',\n",
       " 'twitter',\n",
       " 'account',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenize\n",
    "sample_data_tokenized = word_tokenize(sample_data_lower)\n",
    "sample_data_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set stopwords as english\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today',\n",
       " 'happy',\n",
       " 'tell',\n",
       " 'completed',\n",
       " 'project',\n",
       " 'nlp',\n",
       " '.',\n",
       " 'use',\n",
       " 'nlp',\n",
       " 'extracted',\n",
       " 'information',\n",
       " 'twitter',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " '.',\n",
       " 'used',\n",
       " 'data',\n",
       " 'segment',\n",
       " 'tweets',\n",
       " 'positive',\n",
       " ',',\n",
       " 'negative',\n",
       " 'neutral',\n",
       " 'tweets',\n",
       " '.',\n",
       " 'future',\n",
       " 'use',\n",
       " 'analyze',\n",
       " 'comments',\n",
       " 'twitter',\n",
       " 'account',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's remove stopwords\n",
    "filtered_data = [word for word in sample_data_tokenized if word not in stop_words]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_Learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
